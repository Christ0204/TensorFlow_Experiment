{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  《TensorFlow架构》实验一 TensorFlow安装及编译"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、实验目的\n",
    "1. 掌握TensorFlow的安装与编译方法。\n",
    "2. 掌握TensorFlow的基本语法和数据类型；\n",
    "3. 以CSV文件为例，掌握TensorFlow的数据读取方法；\n",
    "4. 掌握保存和加载TensorFlow模型的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、实验环境\n",
    "1. 笔记本电脑，Linux或MacOs系统；\n",
    "2. Python3.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、实验内容\n",
    "1. 安装TensorFlow 2.0；\n",
    "2. 安装numpy,Matplotlib等Python模块；\n",
    "3. 打开Python，完成TensorFlow基本语法及数据类型的练习操作；\n",
    "4. 下载pima-indians-diabetes.csv文件，读取数据并对数据进行初步了解；\n",
    "5. 用TensorFlow的Keras模型完成对数据的分析；\n",
    "6. 保存模型为静态文件，打开模型文件并对测试数据进行评估。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、主要实验步骤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4.1安装TensorFlow2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 安装numpy,Matplotlib等Python模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方法同上，过程省略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3  熟悉下列TensorFlow常用基本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.1数据类型转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(12.3, shape=(), dtype=float32)\n",
      "tf.Tensor(12.300000190734863, shape=(), dtype=float64)\n",
      "tf.Tensor(12.3, shape=(), dtype=float32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int64)\n",
      "tf.Tensor([1 2], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf1\n",
    "tf1.disable_v2_behavior\n",
    "import numpy as np\n",
    "string = '12.3'\n",
    "n1 = tf1.string_to_number(string, out_type=None, name=None)\n",
    "print(n1)\n",
    "\n",
    "x = 12.3\n",
    "d1 = tf1.to_double(x, name='ToDouble') \n",
    "print(d1)\n",
    "\n",
    "f1 = tf1.to_float(x, name='ToFloat') \n",
    "print(f1)\n",
    "\n",
    "i1 = tf1.to_int32(x, name='ToInt32') \n",
    "print(i1)\n",
    "i2 = tf1.to_int64(x, name='ToInt64') \n",
    "print(i2)\n",
    "\n",
    "a = [1.8, 2.2]\n",
    "i3 = tf.cast(a, tf.int32)\n",
    "print(i3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=12, shape=(), dtype=int32, numpy=4>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[[1,2],[3,4]]\n",
    "tf.size(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.shape(input, name=None) 返回数据的shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2 2 3], shape=(3,), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=39, shape=(3,), dtype=int32, numpy=array([2, 2, 3], dtype=int32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=[\n",
    "    [[1,1,1],[2,2,2]],\n",
    "    [[3,3,3],[4,4,4]]\n",
    "    ]\n",
    "print(tf.shape(t))\n",
    "tf.shape(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2 2 3], shape=(3,), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=33, shape=(3,), dtype=int32, numpy=array([2, 2, 3], dtype=int32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt=tf.Variable(t,name='tt')\n",
    "print(tf.shape(tt))\n",
    "tf.shape(tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.rank(input, name=None) 返回tensor的rank 注意：tensor的rank表示一个tensor需要的索引数目来唯一表示,任何一个元素 也就是通常所说的 “order”, “degree”或"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=35, shape=(), dtype=int32, numpy=3>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.rank(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.reshape(tensor, shape, name=None) 改变tensor的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([9], shape=(1,), dtype=int32)\n",
      "tf.Tensor([3 3], shape=(2,), dtype=int32)\n",
      "tf.Tensor([3 3], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "t=[1,2,3,4,5,6,7,8,9]\n",
    "print(tf.shape(t))\n",
    "\n",
    "t2=tf.reshape(t,[3,3])\n",
    "print(tf.shape(t2))\n",
    "\n",
    "t3=tf.reshape(t,[3,-1])\n",
    "print(tf.shape(t3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.expand_dims(input, dim, name=None) 插入维度1进入一个tensor中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " tf.Tensor([1 2], shape=(2,), dtype=int32)\n",
      "\n",
      " tf.Tensor([2 1], shape=(2,), dtype=int32)\n",
      "\n",
      " tf.Tensor([2 1], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "t=[2,3]\n",
    "\n",
    "t_123=[print('\\n',tf.shape(tf.expand_dims(t,i)))  for  i  in    [0, 1, -1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]], shape=(2, 3, 5), dtype=float32)\n",
      "\n",
      " tf.Tensor([1 2 3 5], shape=(4,), dtype=int32)\n",
      "\n",
      " tf.Tensor([2 3 1 5], shape=(4,), dtype=int32)\n",
      "\n",
      " tf.Tensor([2 3 5 1], shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "t4=tf.ones([2,3,5])\n",
    "print(t4)\n",
    "\n",
    "t_567=[print('\\n',tf.shape(tf.expand_dims(t4,i)))  for  i  in    [0, 2, 3]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.slice(input_, begin, size, name=None) 对tensor进行切片操作,其中size[i] = input.dim_size(i) - begin[i]该操作要求 0 <= begin[i] <= begin[i] + size[i] <= Di for i in [0, n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[[3 3 3]]], shape=(1, 1, 3), dtype=int32)\n",
      "===========\n",
      "tf.Tensor(\n",
      "[[[3 3 3]\n",
      "  [4 4 4]]], shape=(1, 2, 3), dtype=int32)\n",
      "===========\n",
      "tf.Tensor(\n",
      "[[[3 3 3]]\n",
      "\n",
      " [[5 5 5]]], shape=(2, 1, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "t = [\n",
    "       [[1, 1, 1], [2, 2, 2]],\n",
    "       [[3, 3, 3], [4, 4, 4]],\n",
    "       [[5, 5, 5], [6, 6, 6]]\n",
    "      ]\n",
    "\n",
    "t1 = tf.slice(t, [1, 0, 0], [1, 1, 3])\n",
    "t2 = tf.slice(t, [1, 0, 0], [1, 2, 3])\n",
    "t3 = tf.slice(t, [1, 0, 0], [2, 1, 3])\n",
    "\n",
    "print(t1 , t2, t3,sep='\\n===========\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - tf.split(\n",
    "    value,\n",
    "    num_or_size_splits,\n",
    "    axis=0,\n",
    "    num=None,\n",
    "    name='split'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 5 10], shape=(2,), dtype=int32)\n",
      "tf.Tensor([ 5 10], shape=(2,), dtype=int32)\n",
      "tf.Tensor([ 5 10], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "t=tf.ones([5,30])\n",
    "\n",
    "t_123=tf.split(t,num_or_size_splits=3,axis=1)\n",
    "\n",
    "t_123_shape=[print(tf.shape(tt)) for tt in t_123]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.concat(\n",
    "    values,\n",
    "    axis,\n",
    "    name='concat') 沿着某一维度连结tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]], shape=(4, 3), dtype=int32)\n",
      "===\n",
      "tf.Tensor(\n",
      "[[ 1  2  3  7  8  9]\n",
      " [ 4  5  6 10 11 12]], shape=(2, 6), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "t1 = [[1, 2, 3], [4, 5, 6]]\n",
    "t2 = [[7, 8, 9], [10, 11, 12]]\n",
    "\n",
    "t3 = tf.concat([t1, t2], 0) \n",
    "t4 = tf.concat([t1, t2], 1) \n",
    "\n",
    "print(t3,t4,sep='\\n===\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=407, shape=(2, 2, 4), dtype=int32, numpy=\n",
       "array([[[ 1,  2,  7,  4],\n",
       "        [ 2,  3,  8,  4]],\n",
       "\n",
       "       [[ 4,  4,  2, 10],\n",
       "        [ 5,  3, 15, 11]]], dtype=int32)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = [\n",
    "          [[1, 2], [2, 3]], \n",
    "          [[4, 4], [5, 3]]\n",
    "        ]\n",
    "t2 = [\n",
    "          [[7, 4], [8, 4]], \n",
    "          [[2, 10], [15, 11]]\n",
    "        ]\n",
    "tf.concat([t1, t2], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.pack(values, axis=0, name=’pack’) 将一系列rank-R的tensor打包为一个rank-(R+1)的tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 4],\n",
       "       [2, 5],\n",
       "       [3, 6]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=[1,4];  y=[2,5]; z=[3,6]\n",
    "\n",
    "# print(tf.pack([x,y,z],axis=0))\n",
    "# print(tf.pack([x,y,z],axis=1))\n",
    "\n",
    "# AttributeError: module 'tensorflow_core.compat.v1' has no attribute 'pack'\n",
    "np.asarray([x, y, z])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.reverse(tensor, dims, name=None) 沿着某维度进行序列反转其中dim为列表，元素为bool型，size等于rank(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[ 3  2  1  0]\n",
      "   [ 7  6  5  4]\n",
      "   [11 10  9  8]]\n",
      "\n",
      "  [[15 14 13 12]\n",
      "   [19 18 17 16]\n",
      "   [23 22 21 20]]]], shape=(1, 2, 3, 4), dtype=int32)\n",
      "=======\n",
      "tf.Tensor(\n",
      "[[[[12 13 14 15]\n",
      "   [16 17 18 19]\n",
      "   [20 21 22 23]]\n",
      "\n",
      "  [[ 0  1  2  3]\n",
      "   [ 4  5  6  7]\n",
      "   [ 8  9 10 11]]]], shape=(1, 2, 3, 4), dtype=int32)\n",
      "=======\n",
      "tf.Tensor(\n",
      "[[[[ 8  9 10 11]\n",
      "   [ 4  5  6  7]\n",
      "   [ 0  1  2  3]]\n",
      "\n",
      "  [[20 21 22 23]\n",
      "   [16 17 18 19]\n",
      "   [12 13 14 15]]]], shape=(1, 2, 3, 4), dtype=int32)\n",
      "=======\n"
     ]
    }
   ],
   "source": [
    "t = [\n",
    "        [   [[ 0, 1, 2, 3],[ 4, 5, 6, 7],[ 8, 9, 10, 11]],\n",
    "            [[12, 13, 14, 15], [16, 17, 18, 19],[20, 21, 22, 23]]\n",
    "        ]\n",
    "      ]\n",
    "# tensor 't' shape is [1, 2, 3, 4]\n",
    "dims=[\n",
    "             [[3],[-1]],\n",
    "             [[1],[-3]],\n",
    "             [[2],[-2]]\n",
    "            ]\n",
    "\n",
    "t_rev=[print(tf.reverse(t,dim[0]),end='\\n=======\\n') for dim in dims]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.transpose(a, perm=None, name=’transpose’) 调换tensor的维度顺序 按照列表perm的维度排列调换tensor顺序，如未定义，则perm为(n-1…0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]], shape=(3, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]], shape=(3, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "t = [[1, 2, 3],[4, 5, 6]]\n",
    "t1 = tf.transpose(t)\n",
    "t2 = tf.transpose(t, perm=[1, 0])\n",
    "print(t1)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.gather(params, indices, validate_indices=None, name=None) 合并索引indices所指示params中的切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [5 6]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "t = [[1, 2,],[3,4],[ 5, 6]]\n",
    "t_gather=tf.gather(t,[0,2])\n",
    "print(t_gather)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.one_hot(indices,depth,on_value=None,off_value=None,axis=None,dtype=None,name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]], shape=(3, 3), dtype=float32)\n",
      "\n",
      "tf.Tensor(\n",
      "[[5. 0. 0.]\n",
      " [0. 0. 5.]\n",
      " [0. 0. 0.]\n",
      " [0. 5. 0.]], shape=(4, 3), dtype=float32)\n",
      "\n",
      "tf.Tensor(\n",
      "[[[1. 0. 0.]\n",
      "  [0. 0. 1.]]\n",
      "\n",
      " [[0. 1. 0.]\n",
      "  [0. 0. 0.]]], shape=(2, 2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "indices = [0, 1, 2]\n",
    "depth = 3\n",
    "t1 = tf.one_hot(indices, depth) \n",
    "indices = [0, 2, -1, 1]\n",
    "depth = 3\n",
    "t2 = tf.one_hot(indices, depth,on_value=5.0, off_value=0.0,axis=-1) \n",
    "indices = [[0, 2], [1, -1]]\n",
    "depth = 3\n",
    "t3 = tf.one_hot(indices, depth,on_value=1.0, off_value=0.0,axis=-1) \n",
    "\n",
    "print(t1,t2,t3,sep='\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  tf.unique(x, out_idx=tf.dtypes.int32,name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique(y=<tf.Tensor: id=478, shape=(5,), dtype=int32, numpy=array([1, 2, 4, 7, 8], dtype=int32)>, idx=<tf.Tensor: id=479, shape=(9,), dtype=int32, numpy=array([0, 0, 1, 2, 2, 2, 3, 4, 4], dtype=int32)>)\n",
      "tf.Tensor([1 2 4 7 8], shape=(5,), dtype=int32)\n",
      "tf.Tensor([0 0 1 2 2 2 3 4 4], shape=(9,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "t = [1, 1, 2, 4, 4, 4, 7, 8, 8]\n",
    "z= tf.unique(t)\n",
    "y, idx = tf.unique(t)\n",
    "print(z)\n",
    "print(y)\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.zeros(shape,dtype=tf.dtypes.float32,name=None)\n",
    "- tf.diag(diagonal, name=None)tf.trace(x, name=None)\n",
    "- tf.matrix_determinant(input, name=None)\n",
    "- tf.matrix_inverse(input, adjoint=None, name=None) \n",
    "- tf.transpose(a, perm=None, name='transpose')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=62, shape=(2, 3), dtype=float32, numpy=\n",
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros=tf.zeros([2,3])\n",
    "zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[1. 0.]\n",
      "   [0. 0.]]\n",
      "\n",
      "  [[0. 2.]\n",
      "   [0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0.]\n",
      "   [3. 0.]]\n",
      "\n",
      "  [[0. 0.]\n",
      "   [0. 4.]]]], shape=(2, 2, 2, 2), dtype=float32)\n",
      "===========\n",
      "tf.Tensor(-2.0, shape=(), dtype=float32)\n",
      "===========\n",
      "tf.Tensor(5.0, shape=(), dtype=float32)\n",
      "===========\n",
      "tf.Tensor(\n",
      "[[-2.0000002   1.0000001 ]\n",
      " [ 1.5000001  -0.50000006]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "aa=tf.Variable([[1.,2.],[3.,4.]])\n",
    "diag=tf1.diag(aa)            #将矩阵中的元素转换为新矩阵对角线上的元素\n",
    "md=tf1.matrix_determinant(aa)   # 矩阵行列式的值\n",
    "trace=tf1.trace(aa)        #对角线上元素之和\n",
    "mi=tf1.matrix_inverse(aa)        #矩阵求逆\n",
    "print(diag,md,trace,mi,sep='\\n===========\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]], shape=(3, 2), dtype=int32) \n",
      "\n",
      "\n",
      "tf.Tensor(\n",
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]], shape=(3, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "t = [[1, 2, 3],[4, 5, 6]]\n",
    "t1 = tf.transpose(t)\n",
    "t2 = tf.transpose(t, perm=[1, 0])\n",
    "print(t1,'\\n\\n')\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.2 矩阵操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.matmul(a,b,transpose_a=False,transpose_b=False,a_is_sparse=False,b_is_sparse=False,name=None)\n",
    "矩阵相乘\n",
    "- tf.complex(real, imag, name=None) 将两实数转换为复数形式\n",
    "- tf.complex_abs(x, name=None) 计算复数的绝对值，即长度。\n",
    "- tf.conj(input, name=None) 计算共轭复数\n",
    "- tf.imag(input, name=None)\n",
    "- tf.real(input, name=None)\n",
    "- tf.fft(input, name=None)\n",
    "- tf.eye(num_rows,num_columns=None,batch_shape=None,dtype=tf.dtypes.float32,name=None)\n",
    "- tf.fill(dims,value,name=None)\n",
    "- tf.ones(shape,dtype=tf.dtypes.float32,name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=140, shape=(2, 2), dtype=int32, numpy=\n",
       "array([[19, 22],\n",
       "       [43, 50]], dtype=int32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= [\n",
    "         [1,2],\n",
    "         [3,4]\n",
    "     ]\n",
    "b=[\n",
    "       [5,6],\n",
    "       [7,8]\n",
    "     ]\n",
    "ab_matmul=tf.matmul(a,b)\n",
    "ab_matmul\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor((1+2j), shape=(), dtype=complex64)\n",
      "========\n",
      "tf.Tensor(2.236068, shape=(), dtype=float32)\n",
      "========\n",
      "tf.Tensor((1-2j), shape=(), dtype=complex64)\n",
      "========\n",
      "tf.Tensor(2.0, shape=(), dtype=float32)\n",
      "========\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#复数操作\n",
    "a=1.;  b=2.\n",
    "ab_cp=tf.complex(a,b)\n",
    "ab_cp_abs=tf.abs(ab_cp)        \n",
    "ab_cp_conj=tf.math.conj(ab_cp)\n",
    "ab_cp_imag=tf.math.imag(ab_cp)\n",
    "ab_cp_real=tf.math.real(ab_cp)\n",
    "\n",
    "print(ab_cp,ab_cp_abs,ab_cp_conj,ab_cp_imag,ab_cp_real,sep='\\n========\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=207, shape=(2, 2), dtype=complex64, numpy=\n",
       "array([[ 3.+0.j, -1.+0.j],\n",
       "       [ 7.+0.j, -1.+0.j]], dtype=complex64)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= [\n",
    "         [1,2],\n",
    "         [3,4]\n",
    "     ]\n",
    "a_fft=tf1.fft(a)   #快速傅里叶变换\n",
    "a_fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 0.]], shape=(3, 2), dtype=float32)\n",
      "====\n",
      "tf.Tensor(\n",
      "[[21]\n",
      " [21]], shape=(2, 1), dtype=int32)\n",
      "====\n",
      "tf.Tensor(\n",
      "[[1. 1.]\n",
      " [1. 1.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "eye_d=tf.eye(3,2,dtype=tf.dtypes.float32)      #单位矩阵\n",
    "fill_d=tf.fill(dims=[2,1],value=21,name=None)\n",
    "ones_d=tf.ones(shape=[2,2],dtype=tf.dtypes.float32,name=None)  #全一矩阵\n",
    "\n",
    "print(eye_d,fill_d,ones_d,sep='\\n====\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.3生成随机张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.random_normal()、\n",
    "- tf.truncated_normal() 产生截断正态分布随机数，取值范围为 [ mean - 2 * stddev, mean + 2 * stddev ]\n",
    "- tf.random_uniform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.15850271 -0.00646496]\n",
      " [-0.54101175 -0.36107492]], shape=(2, 2), dtype=float32)\n",
      "***********************************************************************************************************************\n",
      "tf.Tensor(\n",
      "[[-0.51454467 -0.3176144 ]\n",
      " [ 1.3686976  -1.2122307 ]], shape=(2, 2), dtype=float32)\n",
      "***********************************************************************************************************************\n",
      "tf.Tensor(\n",
      "[[0.951952   0.9437604 ]\n",
      " [0.30550683 0.91686046]], shape=(2, 2), dtype=float32)\n",
      "***********************************************************************************************************************\n",
      "tf.Tensor([1 0 2], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "sp=[2,2]\n",
    "a=tf1.random_normal(sp)       #正态分布随机数\n",
    "b=tf1.truncated_normal(sp) #产生截断正态分布随机数，取值范围为 [ mean - 2 * stddev, mean + 2 * stddev ]\n",
    "c=tf1.random_uniform(sp)   #均匀分布随机数\n",
    "d=tf1.random_shuffle([0,1,2])   #随机地将张量沿其第一维度打乱\n",
    "\n",
    "print(a,b,c,d,sep='\\n'+'*'*119+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 加载读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=numpy.loadtxt('pima-indians-diabetes.csv',delimiter=',')\n",
    "X=dataset[:,0:8]\n",
    "Y=dataset[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 768 samples\n",
      "Epoch 1/100\n",
      "768/768 [==============================] - 1s 791us/sample - loss: 2.7241 - accuracy: 0.5573\n",
      "Epoch 2/100\n",
      "768/768 [==============================] - 0s 132us/sample - loss: 1.2260 - accuracy: 0.5703\n",
      "Epoch 3/100\n",
      "768/768 [==============================] - 0s 161us/sample - loss: 1.0432 - accuracy: 0.5703\n",
      "Epoch 4/100\n",
      "768/768 [==============================] - 0s 149us/sample - loss: 0.8881 - accuracy: 0.5651\n",
      "Epoch 5/100\n",
      "768/768 [==============================] - 0s 144us/sample - loss: 0.8647 - accuracy: 0.5560\n",
      "Epoch 6/100\n",
      "768/768 [==============================] - 0s 156us/sample - loss: 0.7546 - accuracy: 0.6016\n",
      "Epoch 7/100\n",
      "768/768 [==============================] - 0s 133us/sample - loss: 0.7440 - accuracy: 0.5924\n",
      "Epoch 8/100\n",
      "768/768 [==============================] - 0s 153us/sample - loss: 0.7415 - accuracy: 0.5833\n",
      "Epoch 9/100\n",
      "768/768 [==============================] - 0s 142us/sample - loss: 0.7108 - accuracy: 0.6172\n",
      "Epoch 10/100\n",
      "768/768 [==============================] - 0s 135us/sample - loss: 0.6886 - accuracy: 0.6419\n",
      "Epoch 11/100\n",
      "768/768 [==============================] - 0s 139us/sample - loss: 0.6856 - accuracy: 0.6250\n",
      "Epoch 12/100\n",
      "768/768 [==============================] - 0s 144us/sample - loss: 0.6620 - accuracy: 0.6432\n",
      "Epoch 13/100\n",
      "768/768 [==============================] - 0s 143us/sample - loss: 0.6485 - accuracy: 0.6615\n",
      "Epoch 14/100\n",
      "768/768 [==============================] - 0s 141us/sample - loss: 0.6630 - accuracy: 0.6641\n",
      "Epoch 15/100\n",
      "768/768 [==============================] - 0s 148us/sample - loss: 0.6569 - accuracy: 0.6432\n",
      "Epoch 16/100\n",
      "768/768 [==============================] - 0s 158us/sample - loss: 0.6350 - accuracy: 0.6706\n",
      "Epoch 17/100\n",
      "768/768 [==============================] - 0s 155us/sample - loss: 0.6351 - accuracy: 0.6576\n",
      "Epoch 18/100\n",
      "768/768 [==============================] - 0s 227us/sample - loss: 0.6487 - accuracy: 0.6497\n",
      "Epoch 19/100\n",
      "768/768 [==============================] - 0s 197us/sample - loss: 0.6485 - accuracy: 0.6628\n",
      "Epoch 20/100\n",
      "768/768 [==============================] - 0s 142us/sample - loss: 0.6481 - accuracy: 0.6367\n",
      "Epoch 21/100\n",
      "768/768 [==============================] - 0s 157us/sample - loss: 0.6429 - accuracy: 0.6576\n",
      "Epoch 22/100\n",
      "768/768 [==============================] - 0s 138us/sample - loss: 0.6364 - accuracy: 0.6562\n",
      "Epoch 23/100\n",
      "768/768 [==============================] - 0s 150us/sample - loss: 0.6277 - accuracy: 0.6510\n",
      "Epoch 24/100\n",
      "768/768 [==============================] - 0s 150us/sample - loss: 0.6410 - accuracy: 0.6615\n",
      "Epoch 25/100\n",
      "768/768 [==============================] - 0s 156us/sample - loss: 0.6274 - accuracy: 0.6810\n",
      "Epoch 26/100\n",
      "768/768 [==============================] - 0s 140us/sample - loss: 0.6378 - accuracy: 0.6654\n",
      "Epoch 27/100\n",
      "768/768 [==============================] - 0s 145us/sample - loss: 0.6289 - accuracy: 0.6549\n",
      "Epoch 28/100\n",
      "768/768 [==============================] - 0s 156us/sample - loss: 0.6299 - accuracy: 0.6693\n",
      "Epoch 29/100\n",
      "768/768 [==============================] - 0s 156us/sample - loss: 0.6228 - accuracy: 0.6719\n",
      "Epoch 30/100\n",
      "768/768 [==============================] - 0s 162us/sample - loss: 0.6270 - accuracy: 0.6628\n",
      "Epoch 31/100\n",
      "768/768 [==============================] - 0s 220us/sample - loss: 0.6129 - accuracy: 0.6745\n",
      "Epoch 32/100\n",
      "768/768 [==============================] - 0s 169us/sample - loss: 0.6107 - accuracy: 0.6693\n",
      "Epoch 33/100\n",
      "768/768 [==============================] - 0s 156us/sample - loss: 0.6282 - accuracy: 0.6758\n",
      "Epoch 34/100\n",
      "768/768 [==============================] - 0s 165us/sample - loss: 0.5966 - accuracy: 0.6979\n",
      "Epoch 35/100\n",
      "768/768 [==============================] - 0s 206us/sample - loss: 0.6132 - accuracy: 0.6823\n",
      "Epoch 36/100\n",
      "768/768 [==============================] - 0s 195us/sample - loss: 0.5953 - accuracy: 0.6992\n",
      "Epoch 37/100\n",
      "768/768 [==============================] - 0s 145us/sample - loss: 0.6052 - accuracy: 0.6810\n",
      "Epoch 38/100\n",
      "768/768 [==============================] - 0s 181us/sample - loss: 0.6052 - accuracy: 0.6979\n",
      "Epoch 39/100\n",
      "768/768 [==============================] - 0s 203us/sample - loss: 0.6138 - accuracy: 0.6849\n",
      "Epoch 40/100\n",
      "768/768 [==============================] - 0s 163us/sample - loss: 0.6005 - accuracy: 0.6862\n",
      "Epoch 41/100\n",
      "768/768 [==============================] - 0s 183us/sample - loss: 0.5981 - accuracy: 0.6758\n",
      "Epoch 42/100\n",
      "768/768 [==============================] - 0s 193us/sample - loss: 0.5877 - accuracy: 0.7018\n",
      "Epoch 43/100\n",
      "768/768 [==============================] - 0s 152us/sample - loss: 0.6090 - accuracy: 0.6849\n",
      "Epoch 44/100\n",
      "768/768 [==============================] - 0s 159us/sample - loss: 0.5994 - accuracy: 0.6875\n",
      "Epoch 45/100\n",
      "768/768 [==============================] - 0s 153us/sample - loss: 0.5760 - accuracy: 0.7122\n",
      "Epoch 46/100\n",
      "768/768 [==============================] - 0s 136us/sample - loss: 0.5694 - accuracy: 0.7292\n",
      "Epoch 47/100\n",
      "768/768 [==============================] - 0s 149us/sample - loss: 0.6003 - accuracy: 0.6966\n",
      "Epoch 48/100\n",
      "768/768 [==============================] - 0s 152us/sample - loss: 0.5857 - accuracy: 0.6888\n",
      "Epoch 49/100\n",
      "768/768 [==============================] - 0s 147us/sample - loss: 0.5736 - accuracy: 0.6875\n",
      "Epoch 50/100\n",
      "768/768 [==============================] - 0s 148us/sample - loss: 0.5815 - accuracy: 0.7083\n",
      "Epoch 51/100\n",
      "768/768 [==============================] - 0s 138us/sample - loss: 0.5704 - accuracy: 0.7070\n",
      "Epoch 52/100\n",
      "768/768 [==============================] - 0s 155us/sample - loss: 0.5882 - accuracy: 0.6953\n",
      "Epoch 53/100\n",
      "768/768 [==============================] - 0s 166us/sample - loss: 0.5670 - accuracy: 0.7083\n",
      "Epoch 54/100\n",
      "768/768 [==============================] - 0s 141us/sample - loss: 0.5768 - accuracy: 0.7018\n",
      "Epoch 55/100\n",
      "768/768 [==============================] - 0s 139us/sample - loss: 0.5703 - accuracy: 0.7174\n",
      "Epoch 56/100\n",
      "768/768 [==============================] - 0s 127us/sample - loss: 0.5798 - accuracy: 0.7057\n",
      "Epoch 57/100\n",
      "768/768 [==============================] - 0s 128us/sample - loss: 0.5821 - accuracy: 0.6992\n",
      "Epoch 58/100\n",
      "768/768 [==============================] - 0s 127us/sample - loss: 0.5668 - accuracy: 0.7174\n",
      "Epoch 59/100\n",
      "768/768 [==============================] - 0s 133us/sample - loss: 0.5624 - accuracy: 0.7240\n",
      "Epoch 60/100\n",
      "768/768 [==============================] - 0s 124us/sample - loss: 0.5662 - accuracy: 0.7305\n",
      "Epoch 61/100\n",
      "768/768 [==============================] - 0s 162us/sample - loss: 0.5597 - accuracy: 0.7188\n",
      "Epoch 62/100\n",
      "768/768 [==============================] - 0s 227us/sample - loss: 0.5436 - accuracy: 0.7214\n",
      "Epoch 63/100\n",
      "768/768 [==============================] - 0s 143us/sample - loss: 0.5585 - accuracy: 0.7201\n",
      "Epoch 64/100\n",
      "768/768 [==============================] - 0s 151us/sample - loss: 0.5503 - accuracy: 0.7161\n",
      "Epoch 65/100\n",
      "768/768 [==============================] - 0s 252us/sample - loss: 0.5634 - accuracy: 0.7057\n",
      "Epoch 66/100\n",
      "768/768 [==============================] - 0s 233us/sample - loss: 0.5588 - accuracy: 0.7174\n",
      "Epoch 67/100\n",
      "768/768 [==============================] - 0s 147us/sample - loss: 0.5576 - accuracy: 0.7305\n",
      "Epoch 68/100\n",
      "768/768 [==============================] - 0s 166us/sample - loss: 0.5556 - accuracy: 0.7188\n",
      "Epoch 69/100\n",
      "768/768 [==============================] - 0s 143us/sample - loss: 0.5452 - accuracy: 0.7422\n",
      "Epoch 70/100\n",
      "768/768 [==============================] - 0s 149us/sample - loss: 0.5440 - accuracy: 0.7227\n",
      "Epoch 71/100\n",
      "768/768 [==============================] - 0s 205us/sample - loss: 0.5397 - accuracy: 0.7448\n",
      "Epoch 72/100\n",
      "768/768 [==============================] - 0s 162us/sample - loss: 0.5366 - accuracy: 0.7240\n",
      "Epoch 73/100\n",
      "768/768 [==============================] - 0s 137us/sample - loss: 0.5418 - accuracy: 0.7253\n",
      "Epoch 74/100\n",
      "768/768 [==============================] - 0s 146us/sample - loss: 0.5516 - accuracy: 0.7214\n",
      "Epoch 75/100\n",
      "768/768 [==============================] - 0s 154us/sample - loss: 0.5477 - accuracy: 0.7266\n",
      "Epoch 76/100\n",
      "768/768 [==============================] - 0s 162us/sample - loss: 0.5463 - accuracy: 0.7331\n",
      "Epoch 77/100\n",
      "768/768 [==============================] - 0s 147us/sample - loss: 0.5462 - accuracy: 0.7357\n",
      "Epoch 78/100\n",
      "768/768 [==============================] - 0s 162us/sample - loss: 0.5474 - accuracy: 0.7253\n",
      "Epoch 79/100\n",
      "768/768 [==============================] - 0s 134us/sample - loss: 0.5367 - accuracy: 0.7240\n",
      "Epoch 80/100\n",
      "768/768 [==============================] - 0s 146us/sample - loss: 0.5428 - accuracy: 0.7344\n",
      "Epoch 81/100\n",
      "768/768 [==============================] - 0s 164us/sample - loss: 0.5353 - accuracy: 0.7305\n",
      "Epoch 82/100\n",
      "768/768 [==============================] - 0s 147us/sample - loss: 0.5393 - accuracy: 0.7331\n",
      "Epoch 83/100\n",
      "768/768 [==============================] - 0s 154us/sample - loss: 0.5451 - accuracy: 0.7279\n",
      "Epoch 84/100\n",
      "768/768 [==============================] - 0s 146us/sample - loss: 0.5422 - accuracy: 0.7422\n",
      "Epoch 85/100\n",
      "768/768 [==============================] - 0s 161us/sample - loss: 0.5527 - accuracy: 0.7188\n",
      "Epoch 86/100\n",
      "768/768 [==============================] - 0s 202us/sample - loss: 0.5402 - accuracy: 0.7292\n",
      "Epoch 87/100\n",
      "768/768 [==============================] - 0s 253us/sample - loss: 0.5428 - accuracy: 0.7331\n",
      "Epoch 88/100\n",
      "768/768 [==============================] - 0s 145us/sample - loss: 0.5421 - accuracy: 0.7253\n",
      "Epoch 89/100\n",
      "768/768 [==============================] - 0s 159us/sample - loss: 0.5315 - accuracy: 0.7344\n",
      "Epoch 90/100\n",
      "768/768 [==============================] - 0s 145us/sample - loss: 0.5342 - accuracy: 0.7487\n",
      "Epoch 91/100\n",
      "768/768 [==============================] - 0s 134us/sample - loss: 0.5347 - accuracy: 0.7409\n",
      "Epoch 92/100\n",
      "768/768 [==============================] - 0s 131us/sample - loss: 0.5247 - accuracy: 0.7357\n",
      "Epoch 93/100\n",
      "768/768 [==============================] - 0s 134us/sample - loss: 0.5379 - accuracy: 0.7318\n",
      "Epoch 94/100\n",
      "768/768 [==============================] - 0s 140us/sample - loss: 0.5295 - accuracy: 0.7188\n",
      "Epoch 95/100\n",
      "768/768 [==============================] - 0s 142us/sample - loss: 0.5271 - accuracy: 0.7370\n",
      "Epoch 96/100\n",
      "768/768 [==============================] - 0s 143us/sample - loss: 0.5462 - accuracy: 0.7227\n",
      "Epoch 97/100\n",
      "768/768 [==============================] - 0s 152us/sample - loss: 0.5318 - accuracy: 0.7344\n",
      "Epoch 98/100\n",
      "768/768 [==============================] - 0s 179us/sample - loss: 0.5312 - accuracy: 0.7266\n",
      "Epoch 99/100\n",
      "768/768 [==============================] - 0s 140us/sample - loss: 0.5287 - accuracy: 0.7357\n",
      "Epoch 100/100\n",
      "768/768 [==============================] - 0s 144us/sample - loss: 0.5394 - accuracy: 0.7344\n"
     ]
    }
   ],
   "source": [
    "#1 define the network\n",
    "model=tf.keras.Sequential()\n",
    "model.add(Dense(16,input_dim=8,activation='elu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(32,activation='elu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(10,activation='elu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "# 2 compile the network \n",
    "model.compile(loss='binary_crossentropy',\n",
    "                             optimizer='adam',\n",
    "                             metrics=['accuracy']\n",
    "                            )\n",
    "#3 fitthe network \n",
    "history=model.fit(X,Y,epochs=100,batch_size=10,verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "768/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 109us/sample - loss: 0.5756 - accuracy: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.51, Accuracy: 75.52\n",
      "Prediction Accuracy: 75.52%\n"
     ]
    }
   ],
   "source": [
    "#4 evaluate the network\n",
    "loss,accuracy=model.evaluate(X,Y,verbose=1)\n",
    "print('\\nLoss: %.2f, Accuracy: %.2f'%(loss,accuracy*100))\n",
    "\n",
    "# make predictions\n",
    "probabilities=model.predict(X)\n",
    "predictions=[float(numpy.round(x)) for x in probabilities]\n",
    "acc=numpy.mean(predictions==Y)\n",
    "print('Prediction Accuracy: %.2f%%'%(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 保存和加载模型 \n",
    "TensorFlow有两种保存模型的方法，一种是只保存模型的权重和偏置，另一种是保存整个模型。基本用法如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f265c603850>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_weights('./save_weights/my_save_weigthts')\n",
    "model.load_weights('./save_weights/my_save_weigthts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')\n",
    "restored_model=tf.keras.models.load_model('./my_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
